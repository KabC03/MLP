// test_function.cpp — generated by AI because I'm too lazy to write a main function



// test_function.cpp — 3-D target demo (x, y → z)
#include <iostream>
#include <fstream>
#include <vector>
#include <cmath>
#include "MLP.h++"
#include "matrix.h++"

using namespace std;
using namespace matrix;
using namespace mlp;

// ───────────────────────── helpers ───────────────────────────────────────────
float relu     (float x)        { return x > 0.f ? x : 0.f; }
float relu_d   (float x)        { return x > 0.f ? 1.f : 0.f; }
float identity (float x)        { return x; }
float identity_d(float)         { return 1.f; }
float mse      (float y, float ŷ) { float d = y - ŷ; return 0.5f * d * d; }
float mse_d    (float y, float ŷ) { return ŷ - y; }

#ifndef M_PI
#define M_PI 3.14159265358979323846
#endif
// ───────────────────── editable experiment block ────────────────────────────
namespace cfg {
    // 2-D target surface: z = sin(3x)·cos(2y) + 0.1 x y
    inline float target(float x, float y) {
        return sinf(x * y);
    }

    // Sampling grid
    constexpr size_t GRID = 50;                 // 100 × 100 = 10 000 samples
    constexpr float  XMIN = -M_PI, XMAX =  M_PI;
    constexpr float  YMIN = -M_PI, YMAX =  M_PI;
    constexpr size_t SAMPLES = GRID * GRID;

    // Network topology (2-d input ➜ 1-d output)
    const vector<size_t> DIMS = {2, 32, 32, 1};

    // Weight / bias initialisation range
    constexpr float INIT_MIN = -0.5f, INIT_MAX = 0.5f;

    // Training hyper-parameters
    constexpr float  LR     = 0.01f;
    constexpr size_t EPOCHS = 1000;

    // Call-backs
    constexpr auto H_ACT     = relu;
    constexpr auto H_ACT_D   = relu_d;
    constexpr auto OUT_ACT   = identity;
    constexpr auto OUT_ACT_D = identity_d;
    constexpr auto LOSS      = mse;
    constexpr auto LOSS_D    = mse_d;
} // namespace cfg
// ─────────────────────────────────────────────────────────────────────────────

int main()
{
    /* 1 ─ Generate dataset on a regular grid */
    vector<float> xs, ys, zs;
    xs.reserve(cfg::SAMPLES);
    ys.reserve(cfg::SAMPLES);
    zs.reserve(cfg::SAMPLES);

    for (size_t ix = 0; ix < cfg::GRID; ++ix) {
        float x = cfg::XMIN + (cfg::XMAX - cfg::XMIN) * ix / (cfg::GRID - 1);
        for (size_t iy = 0; iy < cfg::GRID; ++iy) {
            float y = cfg::YMIN + (cfg::YMAX - cfg::YMIN) * iy / (cfg::GRID - 1);
            xs.push_back(x);
            ys.push_back(y);
            zs.push_back(cfg::target(x, y));
        }
    }

    /* 2 ─ Build network */
    auto dims = cfg::DIMS;    // mutable copy for constructor
    MLP<float> net(dims,
                   cfg::INIT_MIN, cfg::INIT_MAX,
                   cfg::H_ACT, cfg::H_ACT_D,
                   cfg::OUT_ACT, cfg::OUT_ACT_D,
                   cfg::LOSS, cfg::LOSS_D);

    /* 3 ─ Train */
    Matrix<float> in(2,1);    // <-- 2-d input
    Matrix<float> tgt(1,1);
    const size_t S = cfg::SAMPLES;

    for (size_t e = 0; e < cfg::EPOCHS; ++e) {
        float epoch_loss = 0.f;

        for (size_t i = 0; i < S; ++i) {
            in .at(0,0) = xs[i];
            in .at(1,0) = ys[i];
            tgt.at(0,0) = zs[i];

            net.backpropagate(in, tgt, cfg::LR);
            epoch_loss += net.loss(tgt).at(0,0);
        }
        if (e % 50 == 0)
            cout << "Epoch " << e << " | mean L = " << epoch_loss / S << '\n';
    }

    /* 4 ─ Dump predictions */
    ofstream out("./data/out.txt", ios::trunc);
    if (!out) { cerr << "Could not open ./data/out.txt\n"; return 1; }

    Matrix<float> pred;
    for (size_t i = 0; i < S; ++i) {
        in.at(0,0) = xs[i];
        in.at(1,0) = ys[i];
        pred = net.run(in);
        out << xs[i] << ", " << ys[i] << ", " << pred.at(0,0) << ", " << zs[i] << '\n';
    }
    cout << "Finished. Results written to ./data/out.txt\n";

    net.save("net_params.txt");
    return 0;
}





